
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0035)https://cv.snu.ac.kr/research/EOPSN/ -->

<!-- This templete is borrowed from AR-NET  -->

<html><head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
       <link rel="shortcut icon" href="favicon.ico">
       <link rel="icon" href="favicon.ico">

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  .row {
    display: flex;
  }

  .column {
    flex: 50%;
    /* padding: 5px; */
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1EPXKPNLQ9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

        gtag('config', 'G-1EPXKPNLQ9');
</script>
<script type="text/javascript" src="./hidebib.js"></script>
<link href="./css" rel="stylesheet" type="text/css">
    <title>Point-VOS: Pointing Up Video Object Segmentation</title>
    <meta property="og:title" content="Point-VOS: Pointing Up Video Object Segmentation">
    <meta property="og:description" content="">
    <meta property="og:url" content="">
  </head>

  <body data-new-gr-c-s-check-loaded="14.974.0">
        <br>
        <center><span style="font-size:40px;font-weight:bold;color:#182B49">Point-VOS: Pointing Up Video Object Segmentation</span></center> <br><br>

        <table align="center" width="910px">
          <tbody><tr>
            <td align="center" width="120px">
                <center><span style="font-size:20px"><a href="https://www.vision.rwth-aachen.de/person/218/" target="_blank">Idil Esen Zulfikar</a><sup>1,<span style="color:red;">*</span>†</sup></span></center></td>
            <td align="center" width="120px">
                <center><span style="font-size:20px"><a href="https://www.vision.rwth-aachen.de/person/245/" target="_blank">Sabarinath Mahadevan</a><sup>1,<span style="color:red;">*</span>†</sup></span></center></td>
            <td align="center" width="120px">
                <center><span style="font-size:20px"><a href="https://research.google/people/paul-voigtlaender/" target="_blank">Paul Voigtlaender</a><sup>2,<span style="color:red;">*</span></sup> </span></center></td>
            <td align="center" width="120px">
                 <center><span style="font-size:20px"><a href="https://www.vision.rwth-aachen.de/person/1/" target="_blank">Bastian Leibe</a><sup>1</sup> </span></center></td>
          </tr><tr>
         </tr></tbody></table>

        <table align="center" width="600px">
          <tbody><tr>
                <td align="center" width="500px"><center><span style="font-size:20px">RWTH Aachen University<sup>1</sup></span></center></td>
                <td align="center" width="500px"><center><span style="font-size:20px">Google Research<sup>2</sup></span></center></td>
            </tr><tr>

        </tr></tbody>
        </table> <br>

        <table align="center" width="500px">
          <tbody>
            <tr>
            <td align="center" width="150px">
                <center><span style="font-size:14px"><span style="color:red;">*</span>These authors contributed equally to this work.</span></center>
            </td></tr>
            <tr>
            <td align="center" width="150px">
                <center><span style="font-size:14px"><span style="color:black;">†</span>The ordering of the first two authors was determined by a last-minute coin flip.</span></center>
            </td></tr>
          </tbody>
        </table> <br>

        <table align="center" width="900px">
          <tbody><tr>
            <td align="center" width="300px">
            <center><a href="https://www.vision.rwth-aachen.de/" target="_blank"> <img alt="RWTH" src="./assets/figures/logos/rwth.png" width="200"></a></center></td>
            <td align="center" width="300px">
            <center><a href="https://research.google/" target="_blank"> <img alt="Google" src="./assets/figures/logos/google-ai-meta.png" width="300"></a></center></td>
          </tr></tbody>
        </table>
        

        <center id="abstract"><h1>Abstract</h1></center>
        <table align="center" width="1000px">
            <tbody><tr><td width="1000px">
              <center>
                <img src="./assets/figures/teaser.png" height="300px" ></a><br></center>
            </td></tr>
            </tbody>
        </table>
        <br> <br>

        <center>
        <div style="text-align:justify;width:900px">
        Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results. We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points. In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task. We will make our code and annotations available upon acceptance. 
        <br>
        </div>
        </center>
        <br>
      <br>
      <hr>
      <br>
    <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body></html>
